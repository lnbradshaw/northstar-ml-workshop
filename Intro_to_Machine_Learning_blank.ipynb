{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8067f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "\n",
    "plt.rcParams.update({'font.family': 'cmr10', 'font.size': 12})\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['mathtext.fontset'] = 'cm'\n",
    "plt.rcParams['mathtext.rm'] = 'serif'\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['xtick.top'] = True\n",
    "plt.rcParams['ytick.right'] = True\n",
    "plt.rcParams['axes.formatter.use_mathtext'] = True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few useful functions \n",
    "\n",
    "def preview_img(index, data):\n",
    "    \"\"\"\n",
    "    index: int, index of the image to display\n",
    "    \n",
    "    data: array, dataset to diplay an image from\n",
    "    \"\"\"\n",
    "    if index > data.shape[0]:\n",
    "        print(f\"The index you choose must be less than {data.shape[0]}\")\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1,1, figsize=(4,4))\n",
    "        ax.matshow(data[index,...], cmap='gray')\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "def model_accuracy(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c53e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not already installed TensorFlow and Keras, un-comment and run the following lines\n",
    "\n",
    "#! pip install tensorflow\n",
    "# If you have a Mac and the above line didn't work, try the one below\n",
    "#! pip install tensorflow-macos\n",
    "\n",
    "#! pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d899920",
   "metadata": {},
   "source": [
    "# Exploring and preprocessing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3fa219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by loading in data\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(training_data, training_labels), (testing_data, testing_labels) = mnist.load_data()\n",
    "\n",
    "print(training_data.shape, testing_data.shape)\n",
    "print(training_labels.shape, testing_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa232205",
   "metadata": {},
   "source": [
    "By default, the label associated with each image is the value of the digit shown in that image. Since we're only interested in whether or not the handwritten digit is even or odd, we want to adjust the labels so that every even digit is labeled by a zero, and every odd digit is labeled by a one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels[training_labels%2==0] = 0\n",
    "training_labels[training_labels%2==1] = 1\n",
    "testing_labels[testing_labels%2==0] = 0\n",
    "testing_labels[testing_labels%2==1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70fa68e",
   "metadata": {},
   "source": [
    "With the labels sorted, let's make sure that the data matches what we expect. The training (testing) data contains 60,000 (10,000) 28x28 greyscale images of handwritten digits. Pick a few numbers between 0 and 59,999 and plug them into the <tt>preview_img</tt> function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_img(12345, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a04663",
   "metadata": {},
   "source": [
    "For the network we'll be building today, we'll want to reformat each image as a column vector, and then stack all of those column vectors side-by-side into a 'matrix' that looks like $$\\begin{bmatrix} | & | & | & \\ldots & | \\\\ x^{[0]}_{i} & x^{[1]}_{i} & x^{[2]}_{i} & \\ldots & x^{[N]}_{i} \\\\ | & | & | & \\ldots & | \\end{bmatrix}$$ where $x^{[k]}_{i}$ is the intensity of the $i$th pixel in the $k$th image, and $N$ is the total number of images we have (60,000 for the training set, 10,000 for the testing set)\n",
    "\n",
    "There are network that allow us to train directly on images (Convolutional Neural Networks), but we won't be discussing those today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.reshape(:20000, training_data.shape[1]*training_data.shape[2]).T\n",
    "testing_data = testing_data.reshape(-1, testing_data.shape[1]*testing_data.shape[2]).T\n",
    "training_labels = training_labels.reshape(1,:20000)\n",
    "testing_labels = testing_labels.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73040fc",
   "metadata": {},
   "source": [
    "# Defining the Model Architecture\n",
    "\n",
    "We can think of our neural network in the following way:\n",
    "\n",
    "![](Images/black_box_nn.png)\n",
    "\n",
    "where the inner workings are quite literally (for now, at least) a black box. \n",
    "\n",
    "Inside this model, there are a number of $\\textit{layers}$, corresponding to either our input data, the model's output, or some learned representation along the way. We say a layer is \"hidden\" if it isn't the model's input or output, since we typically don't access any of the information in those layers. If it's helpful, you can think of these layers as being column vectors. Our updated picture now looks like:\n",
    "\n",
    "![](Images/nn_arch.png)\n",
    "\n",
    "Each $\\textit{layer}$ consists of a certain number of $\\textit{nodes}$. For our example, the number of nodes in the input layer will be the total number of pixels in the input image. We'll only have a single node in the output layer, since we want our model to learn whether a given input corresponds to one of two classes. \n",
    "\n",
    "The network we'll build today is called a $\\textbf{Dense}$ network since every node in the $k$th layer depends on every node in the $k-1$th layer. Diagramatically, this looks like:\n",
    "\n",
    "![](Images/dense_layout.png)\n",
    "\n",
    "where each circle represents a node in a given layer, and each arrow indicates how much the node in one layer depends on the value of the node in the previous layer.\n",
    "\n",
    "Mathematically, the value of the $i$th node in the $k$th layer is $$z^{[k]}_{i} = \\displaystyle\\sum_{j=1}^{N_{k-1}} w_{ij}z^{[k-1]}_{j} + b_{i}$$ where $w_{ij}$ is a matrix of weights connecting the $i$th node in the $k$th layer to all of the nodes in the $k-1$th layer, and $b_{i}$ is bias parameter that can also be learned. We can then do this operation for every node $i$ in layer $k$ by stacking them in a column. \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} x_{0}^{[k]} \\\\ x_{1}^{[k]} \\\\ \\vdots \\\\ x_{N_{k}}^{[k]}\\end{bmatrix} = \n",
    "\\begin{bmatrix} w_{00} & w_{01} & \\ldots & w_{0 N_{k-1}} \\\\ w_{10} & w_{11}  & \\ldots &  w_{1 N_{k-1}} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{N_{k} 0} & w_{N_{k} 1} & \\ldots &  w_{N_{k} N_{k-1}} \\end{bmatrix}\n",
    "\\begin{bmatrix} x_{0}^{[k-1]} \\\\ x_{1}^{[k-1]} \\\\ \\vdots \\\\ x_{N_{k-1}}^{[k-1]}\\end{bmatrix} + \n",
    "\\begin{bmatrix} b_{0}^{[k]} \\\\ b_{1}^{[k]} \\\\ \\vdots \\\\ b_{N_{k}}^{[k]}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To summarize, the architecture of the network is determined by the the number of layers we use, and the number of nodes in each layer. The number of nodes in the input and output layers will be determined by our data and the task we ultimately want to do, but we are free to set the number of hidden layers as well as the number of nodes in each of those hidden layers. For simplicity, we will create a model with just a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767a7d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_per_layer(input_data, true_labels):\n",
    "    \"\"\"\n",
    "    Define the number of nodes in each layer. The number of nodes in the input and output layer are determined \n",
    "    by our input data and true_labels, respectively. But, we are free to set the number of nodes \n",
    "    in the hidden layer.\n",
    "    \"\"\"\n",
    "    n_inputs = input_data.shape[0]\n",
    "    n_hidden = 12\n",
    "    n_outputs = true_labels.shape[0]\n",
    "    \n",
    "    return (n_inputs, n_hidden, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e88673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_inputs, n_hidden, n_outputs):\n",
    "    \"\"\"\n",
    "    Initialize all of the parameters used to train the model, given the number of nodes in each layer. \n",
    "    W1 will connect our input layer to the hidden layer, W2 will connect the input layer to the output layer. \n",
    "    To start, we will set both bias vectors to 0.\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(n_hidden, n_inputs)*0.01\n",
    "    b1 = np.zeros((n_hidden,1))\n",
    "    W2 = np.random.randn(n_outputs, n_hidden)*0.01\n",
    "    b2 = np.zeros((n_outputs,1))\n",
    "    \n",
    "    parameters = {\"W1\":W1, \"b1\":b1, \"W2\":W2, \"b2\":b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd15ec",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "With the architecture of our model defined, we can take the first step towards training it. Starting from our input data, we'll carry out the necessary operations to produce the model's prediction for each input's corresponding label. \n",
    "\n",
    "The power of machine learning comes from being able to learn non-linearities present in our data. Our current formulation of connecting layers , however, only allows us to learn linear combinations of our data. To correct this, we then need to pass each layer (with the exception of the input layer) through a nonlinear function, called an $\\textit{activation}$ function. \n",
    "\n",
    "For the hidden layer, we'll be using the <tt>tanh</tt> function, though there are other choices we could have made. For the output layer though, we want to make sure that we choose a function who's domain is $[0,1]$ since the labels we want our model to predict are either 0 or 1. We will use the <tt>sigmoid</tt> activation function for the output layer. Note that if we were interested in a different task, we would want to choose a different activation function for the output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126b8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \n",
    "    s = 1/(1+np.exp(-x))\n",
    "    \n",
    "    return s\n",
    "\n",
    "# plot the sigmoid function\n",
    "x = np.linspace(-5,5,101)\n",
    "plt.plot(x, sigmoid(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa15a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(input_data, parameters):\n",
    "    \"\"\"\n",
    "    We know how to link each layer to the next given the number of nodes in the input layer, \n",
    "    hidden layer, and output layer. We want to use the 'tanh' activation function for the hidden layer, and the\n",
    "    'sigmoid' activation function for the output layer.\n",
    "    \"\"\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    Z1 = np.dot(W1, input_data) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = {\"Z1\":Z1, \"A1\":A1, \"Z2\":Z2, \"A2\":A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de650a",
   "metadata": {},
   "source": [
    "For each event, we compute the \n",
    "$\\textit{loss}$, which tells us how far off the network's guess was from the true label. For binary classification, the loss function we often us is the $\\textbf{binary crossentropy}$ $$\\mathcal{L}(\\hat{y}, y) = -y\\ln(\\hat{y}) - (1-y)\\ln(1-\\hat{y})$$ where $y$ is the true label and $\\hat{y}$ is the network's output (what we have been calling $A^{[2]}$). By eye, we can see that if the network's output matches the true label, the loss function will evaluate to zero.\n",
    "\n",
    "We can them compute a related quantity, called the $\\textit{cost}$\n",
    "$$C[W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}] = \\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\mathcal{L}(\\hat{y}_{i}, y_{i})$$ which is just the average loss for each event in our training set and gives us a sense of how well our model is performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd667ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, true_labels, parameters):\n",
    "    \"\"\"\n",
    "    For us, the cost is the average value of the binary crossentropy for each event\n",
    "    \"\"\"\n",
    "    m = true_labels.shape[1]\n",
    "    \n",
    "    cost  = (-1/m)* ( np.dot(true_labels, np.log(A2.T)) + np.dot((1-true_labels), np.log(1-A2.T)) )\n",
    "    cost = np.squeeze(cost) #(n,1) -> (n,)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e93e7f",
   "metadata": {},
   "source": [
    "# Back Propagation\n",
    "\n",
    "The goal of training our neural network is to have its predictions get better and better with each iteration. To do this, we need a way of updating the parameters in our network---the weight matrices and biases---depending on how well our network performed. \n",
    "\n",
    "Given that we rigidly defined the cost function, we know how it depends on every parameter in our network. So, we can work our ways backwards to figure out how we the cost depends on each parameter in the network. Diagramatically, we can think of this in the following way:\n",
    "\n",
    "<img src='Images/back_prop.png'>\n",
    "\n",
    "where $\\frac{\\partial C}{\\partial W^{[2]}}$ tells us how $C$ depends on $W^{[2]}$. To actually compute this, we need to use the chain rule, which gives: \n",
    "$$ \\frac{\\partial C}{\\partial W^{[2]}} = \\frac{\\partial C}{\\partial A^{[2]}} \\frac{\\partial A^{[2]}}{\\partial Z^{[2]}} \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} =: \\textrm{\"dW2\"} $$\n",
    "We'll then need to repeat a similar calculation for $\\textrm{\"db2\"} = \\frac{\\partial C}{\\partial b^{[2]}}$, $\\textrm{\"dW1\"} = \\frac{\\partial C}{\\partial W^{[1]}}$, and $\\textrm{\"db1\"} = \\frac{\\partial C}{\\partial b^{[1]}}$\n",
    "\n",
    "For the sake of time, these gradients are provided below, but try and go back and calculate each one to make sure you understand where they came from (you may find it easier to compute these gradients with respect to the loss (single event), rather than the cose (all events)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4848814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(parameters, cache, input_data, true_labels):\n",
    "    m = input_data.shape[1]\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "\n",
    "    dZ2 = A2 - true_labels\n",
    "    dW2 = (1/m)*np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m)*np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = (1/m)*np.multiply(np.dot(W2.T, dZ2),(1-np.power(A1, 2)))\n",
    "    dW1 = (1/m)*np.dot(dZ1, input_data.T)\n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\":dW1, \"db1\":db1, \"dW2\":dW2, \"db2\":db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb236cb",
   "metadata": {},
   "source": [
    "To actually update all of the parameters, we use a $\\textit{gradient descent}$ algorithm. The easiest way to explain this is diagramatically (image from https://towardsdatascience.com/quick-guide-to-gradient-descent-and-its-variants-97a7afb33add):\n",
    "\n",
    "<img src='Images/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg'>\n",
    "\n",
    "We've computed the gradients of our cost function for each parameter, so we know how each parameter will affect the model's cost function. We then want to take a step of a certain size to a new value of each parameter, with that step being in the direction of the minimum value of the cost function. The size of this step, the $\\textit{learning rate}$, is another hyperparameter that we are free to set ourselves, like the number of hidden layers and the number of nodes in each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209923fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Given the model parameters, gradients, and learning rate, we can update each parameter as \n",
    "    parameter = parameter-learning_rate*gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    parameters = {\"W1\":W1, \"b1\":b1, \"W2\":W2, \"b2\":b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ba3965",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Now that we know how to define the model architecture, propagate our input data forward to get the model's guess at the correct label, and update all of the parameters in our network based on how closely the model's predicted labels were to the true labels, let's put everything together and actually train our neural network.\n",
    "\n",
    "We'll train our network for 1000 $\\textit{epochs}$, or pass throughs of our entire training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628bbcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(input_data, true_labels, n_hidden, num_iterations, learning_rate, print_cost=True):\n",
    "    \"\"\"\n",
    "    Start by defining the layer sizes and initializing the parameters. Then carry out the forward propagation, \n",
    "    compute the loss and accuracy (and append those to separate lists). Compute the gradients, and finally update\n",
    "    the model parameters. Every 100 iterations, print the loss and accuracy\n",
    "    \"\"\"\n",
    "    n_inputs = nodes_per_layer(input_data, true_labels)[0]\n",
    "    n_outputs = nodes_per_layer(input_data, true_labels)[2]\n",
    "    \n",
    "    parameters = initialize_parameters(n_inputs, n_hidden, n_outputs)\n",
    "    cost_list = []\n",
    "    accuracy_list = []\n",
    "    for i in range(num_iterations):\n",
    "        A2, cache = forward_prop(input_data, parameters)\n",
    "        cost = compute_cost(A2, true_labels, parameters)\n",
    "        predictions = (A2 > 0.5)\n",
    "        accuracy = model_accuracy(true_labels[0], predictions[0])\n",
    "        cost_list.append(cost)\n",
    "        accuracy_list.append(accuracy)\n",
    "        grads = back_prop(parameters, cache, input_data, true_labels)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        if print_cost and i%100==0:\n",
    "            print(f'After {i} iterations, the model\\'s cost is {np.round(cost, 5)} and its accuracy is {np.round(accuracy, 2)}%')\n",
    "    \n",
    "    return parameters, cost_list, accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ec1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, input_data):\n",
    "    A2, cache = forward_prop(input_data, parameters)\n",
    "    predictions = (A2 > 0.5)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967de764",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "parameters, cost_list, acc_list = nn_model(training_data, training_labels, n_hidden=12, \n",
    "                                           num_iterations=1001, learning_rate=10, print_cost=True)\n",
    "end = perf_counter()\n",
    "\n",
    "print(f\"Our model took {np.round(end-start, 1)} seconds to train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training cost, accuracy\n",
    "fig, ax = plt.subplots(1,2,figsize=(8,4))\n",
    "\n",
    "ax[0].plot(cost_list)\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Cost')\n",
    "\n",
    "ax[1].plot(acc_list)\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e91bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how accurate is out model when classifying data it hasn't seen before?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7e87c",
   "metadata": {},
   "source": [
    "# Simplifying the process with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfc956c",
   "metadata": {},
   "source": [
    "While the above example is useful for de-mystifying machine learning and gaining an appreciation for what's going on under the hood, it won't be what you use (or want to use) in practice. \n",
    "\n",
    "Keras (https://keras.io) is a powerful, easy-to-use machine learning API built on top of TensorFlow2 (https://www.tensorflow.org). Here, we'll use Keras to build a neural network with an identical architecture, train that model on the same data, and see how it compares to the network we wrote from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63a1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47358565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and train a keras model\n",
    "\n",
    "input_layer = tf.keras.layers.Input((training_data.T.shape[1],))\n",
    "hidden_layer = tf.keras.layers.Dense(4, activation='tanh')(input_layer)\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "keras_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "keras_model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-3), metrics=['acc'])\n",
    "\n",
    "history = keras_model.fit(training_data.T, training_labels.T, verbose=1, validation_split=0.15, \n",
    "                          epochs=50, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how does the model we instantiated with Keras compare to the one we made from scratch?\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(8,4), constrained_layout=True)\n",
    "\n",
    "ax[0].plot(cost_list, label='NN From Scratch')\n",
    "ax[0].plot(history.history['loss'], label='Keras NN')\n",
    "ax[0].set_xlabel('Epoch', fontsize='x-large')\n",
    "ax[0].set_ylabel('Cost', fontsize='x-large')\n",
    "ax[0].legend(loc='upper right', fontsize='large')\n",
    "\n",
    "ax[1].plot(0.01*np.array(accuracy_list), label='NN from Scratch')\n",
    "ax[1].plot(history.history['acc'], label='Keras NN')\n",
    "ax[1].set_xlabel('Epoch', fontsize='x-large')\n",
    "ax[1].set_ylabel('Accuracy', fontsize='x-large')\n",
    "ax[1].legend(loc='lower right', fontsize='large');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069c26b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
